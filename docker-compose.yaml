services:
  rabbitmq:
    image: rabbitmq:management
    restart: always
    ports:
    - ${DATALAB_RABBITMQ_MANAGEMENT_PORT}:15672
    healthcheck:
      test:
      - CMD
      - rabbitmqctl
      - status
      interval: 10s
      retries: 5
      timeout: 5s
      start_period: 30s
  server:
    image: us-central1-docker.pkg.dev/inference-build/inference-images/server:latest
    ports:
    - 8510:8000
    depends_on:
      rabbitmq:
        condition: service_healthy
    volumes:
    - ${DATALAB_INFERENCE_DATA_VOLUME}:/data
    - ${DATALAB_INFERENCE_OUTPUT_VOLUME}:/output
    environment:
      RABBITMQ_HOST: rabbitmq
      DATA_DIR: /data
    restart: unless-stopped
  gpu0-worker:
    image: us-central1-docker.pkg.dev/inference-build/inference-images/combined:latest
    depends_on:
      rabbitmq:
        condition: service_healthy
      server:
        condition: service_started
    volumes:
    - ${DATALAB_INFERENCE_DATA_VOLUME}:/data
    - ${DATALAB_INFERENCE_OUTPUT_VOLUME}:/output
    - /tmp/nvidia-mps-0:/tmp/nvidia-mps
    - /tmp/nvidia-log-0:/tmp/nvidia-log
    environment:
      RABBITMQ_HOST: rabbitmq
      DATA_DIR: /data
      OUTPUT_DIR: /output
      COMPILE_MODELS: ${DATALAB_COMPILE_MODELS}
      CUDA_MPS_PIPE_DIRECTORY: /tmp/nvidia-mps
      CUDA_MPS_LOG_DIRECTORY: /tmp/nvidia-log
      CHUNK_SIZE: ${DATALAB_INFERENCE_CHUNK_SIZE}
      RECOGNITION_BATCH_SIZE: ${DATALAB_INFERENCE_RECOGNITION_BATCH_SIZE}
      DETECTION_BATCH_SIZE: ${DATALAB_INFERENCE_DETECTION_BATCH_SIZE}
      TABLE_REC_BATCH_SIZE: ${DATALAB_INFERENCE_TABLE_REC_BATCH_SIZE}
      LAYOUT_BATCH_SIZE: ${DATALAB_INFERENCE_LAYOUT_BATCH_SIZE}
      OCR_ERROR_BATCH_SIZE: ${DATALAB_INFERENCE_OCR_ERROR_BATCH_SIZE}
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids:
            - '0'
            capabilities:
            - gpu
      replicas: 1
